{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"A Multilayer Perceptron class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers=[3, 3], num_outputs=2):\n",
    "        \"\"\"Constructor for the MLP. Takes the number of inputs,\n",
    "            a variable number of hidden layers, and number of outputs\n",
    "\n",
    "        Arguments:\n",
    "            num_inputs (int): Number of inputs\n",
    "            hidden_layers (list): A list of ints for the hidden layers\n",
    "            num_outputs (int): Number of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # to simplify the representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "\n",
    "        # create random connection weights for the layers, segments the \"layer\" array into the necessary matrices.\n",
    "        weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i + 1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "\n",
    "        # save derivatives per layer\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d = np.zeros((layers[i], layers[i + 1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "\n",
    "        # save activations per layer\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        \"\"\"Computes forward propagation of the network based on input signals.\n",
    "\n",
    "        Args:\n",
    "            inputs (ndarray): Input signals\n",
    "        Returns:\n",
    "            activations (ndarray): Output values\n",
    "        \"\"\"\n",
    "\n",
    "        # the input layer activation is just the input itself\n",
    "        activations = inputs\n",
    "\n",
    "        # save the activations for backpropogation\n",
    "        self.activations[0] = activations\n",
    "\n",
    "        # iterate through the network layers\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # calculate matrix multiplication between previous activation and weight matrix\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # apply sigmoid activation function\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "\n",
    "            # save the activations for backpropogation\n",
    "            self.activations[i + 1] = activations\n",
    "\n",
    "        # return output layer activation\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_propagate(self, error):\n",
    "        \"\"\"Backpropogates an error signal.\n",
    "        Args:\n",
    "            error (ndarray): The error to backprop.\n",
    "        Returns:\n",
    "            error (ndarray): The final error of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate backwards through the network layers\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "\n",
    "            # get activation for previous layer\n",
    "            activations = self.activations[i+1]\n",
    "\n",
    "            # apply sigmoid derivative function\n",
    "            delta = error * self._sigmoid_derivative(activations)\n",
    "\n",
    "            # reshape delta as to have it as a 2d array\n",
    "            delta_re = delta.reshape(delta.shape[0], -1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            current_activations = self.activations[i]\n",
    "\n",
    "            # reshape activations as to have them as a 2d column matrix\n",
    "            current_activations = current_activations.reshape(current_activations.shape[0],-1)\n",
    "\n",
    "            # save derivative after applying matrix multiplication\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_re)\n",
    "\n",
    "            # backpropogate the next error\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        \"\"\"Trains model running forward prop and backprop\n",
    "        Args:\n",
    "            inputs (ndarray): X\n",
    "            targets (ndarray): Y\n",
    "            epochs (int): Num. epochs we want to train the network for\n",
    "            learning_rate (float): Step to apply to gradient descent\n",
    "        \"\"\"\n",
    "        # now enter the training loop\n",
    "        for i in range(epochs):\n",
    "            sum_errors = 0\n",
    "\n",
    "            # iterate through all the training data\n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "\n",
    "                # activate the network!\n",
    "                output = self.forward_propagate(input)\n",
    "\n",
    "                error = target - output\n",
    "\n",
    "                self.back_propagate(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # keep track of the MSE for reporting later\n",
    "                sum_errors += self._mse(target, output)\n",
    "\n",
    "            # Epoch complete, report the training error\n",
    "            print(\"Error: {} at epoch {}\".format(sum_errors / len(items), i+1))\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        print(\"=====\")\n",
    "\n",
    "\n",
    "    def gradient_descent(self, learningRate=1):\n",
    "        \"\"\"Learns by descending the gradient\n",
    "        Args:\n",
    "            learningRate (float): How fast to learn.\n",
    "        \"\"\"\n",
    "        # update the weights by stepping down the gradient\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learningRate\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\n",
    "        Args:\n",
    "            x (float): Value to be processed\n",
    "        Returns:\n",
    "            y (float): Output\n",
    "        \"\"\"\n",
    "\n",
    "        y = 1.0 / (1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        \"\"\"Sigmoid derivative function\n",
    "        Args:\n",
    "            x (float): Value to be processed\n",
    "        Returns:\n",
    "            y (float): Output\n",
    "        \"\"\"\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "\n",
    "    def _mse(self, target, output):\n",
    "        \"\"\"Mean Squared Error loss function\n",
    "        Args:\n",
    "            target (ndarray): The ground trut\n",
    "            output (ndarray): The predicted values\n",
    "        Returns:\n",
    "            (float): Output\n",
    "        \"\"\"\n",
    "        return np.average((target - output) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0.]), array([0., 0., 0., 0., 0.]), array([0., 0., 0.])]\n",
      "Error: 0.04131805201407656 at epoch 1\n",
      "Error: 0.027360095010529224 at epoch 2\n",
      "Error: 0.0265835739394825 at epoch 3\n",
      "Error: 0.026471570196743864 at epoch 4\n",
      "Error: 0.02636625702469146 at epoch 5\n",
      "Error: 0.02623662627380885 at epoch 6\n",
      "Error: 0.026073380901251483 at epoch 7\n",
      "Error: 0.025866513182753546 at epoch 8\n",
      "Error: 0.025604058581643722 at epoch 9\n",
      "Error: 0.025272511981196677 at epoch 10\n",
      "Error: 0.024858383797652338 at epoch 11\n",
      "Error: 0.024351195920684333 at epoch 12\n",
      "Error: 0.023747545497484302 at epoch 13\n",
      "Error: 0.023054692656852577 at epoch 14\n",
      "Error: 0.02229161858718809 at epoch 15\n",
      "Error: 0.021486621565717404 at epoch 16\n",
      "Error: 0.02067237906359426 at epoch 17\n",
      "Error: 0.019880336106421405 at epoch 18\n",
      "Error: 0.01913609136369977 at epoch 19\n",
      "Error: 0.018456826783808222 at epoch 20\n",
      "Error: 0.01785101913238143 at epoch 21\n",
      "Error: 0.01731987663458143 at epoch 22\n",
      "Error: 0.0168595526008531 at epoch 23\n",
      "Error: 0.016463315983684133 at epoch 24\n",
      "Error: 0.016123242235561035 at epoch 25\n",
      "Error: 0.015831329590886078 at epoch 26\n",
      "Error: 0.015580130757099615 at epoch 27\n",
      "Error: 0.015363043868425096 at epoch 28\n",
      "Error: 0.015174392623450241 at epoch 29\n",
      "Error: 0.015009390446720214 at epoch 30\n",
      "Error: 0.014864049841391051 at epoch 31\n",
      "Error: 0.014735072897988066 at epoch 32\n",
      "Error: 0.014619742289537966 at epoch 33\n",
      "Error: 0.014515821971025964 at epoch 34\n",
      "Error: 0.014421471054845503 at epoch 35\n",
      "Error: 0.01433517129308878 at epoch 36\n",
      "Error: 0.014255667140853707 at epoch 37\n",
      "Error: 0.01418191679911498 at epoch 38\n",
      "Error: 0.014113052521947687 at epoch 39\n",
      "Error: 0.014048348581187666 at epoch 40\n",
      "Error: 0.013987195480289628 at epoch 41\n",
      "Error: 0.013929079229457067 at epoch 42\n",
      "Error: 0.013873564702782516 at epoch 43\n",
      "Error: 0.013820282281177357 at epoch 44\n",
      "Error: 0.013768917138373976 at epoch 45\n",
      "Error: 0.013719200652255275 at epoch 46\n",
      "Error: 0.01367090352337301 at epoch 47\n",
      "Error: 0.013623830260661307 at epoch 48\n",
      "Error: 0.013577814755009321 at epoch 49\n",
      "Error: 0.013532716708283403 at epoch 50\n",
      "Error: 0.01348841872201787 at epoch 51\n",
      "Error: 0.013444823879398203 at epoch 52\n",
      "Error: 0.013401853679015448 at epoch 53\n",
      "Error: 0.01335944620138388 at epoch 54\n",
      "Error: 0.013317554411037283 at epoch 55\n",
      "Error: 0.013276144519153248 at epoch 56\n",
      "Error: 0.01323519435438196 at epoch 57\n",
      "Error: 0.013194691712452645 at epoch 58\n",
      "Error: 0.013154632677173818 at epoch 59\n",
      "Error: 0.0131150199252219 at epoch 60\n",
      "Error: 0.013075861043126217 at epoch 61\n",
      "Error: 0.013037166895834337 at epoch 62\n",
      "Error: 0.01299895009141783 at epoch 63\n",
      "Error: 0.012961223585779435 at epoch 64\n",
      "Error: 0.012923999465286183 at epoch 65\n",
      "Error: 0.012887287935322999 at epoch 66\n",
      "Error: 0.012851096530452476 at epoch 67\n",
      "Error: 0.012815429548894352 at epoch 68\n",
      "Error: 0.012780287701947944 at epoch 69\n",
      "Error: 0.012745667958954668 at epoch 70\n",
      "Error: 0.012711563561149905 at epoch 71\n",
      "Error: 0.01267796417352474 at epoch 72\n",
      "Error: 0.012644856142447652 at epoch 73\n",
      "Error: 0.012612222827836708 at epoch 74\n",
      "Error: 0.012580044981520804 at epoch 75\n",
      "Error: 0.012548301147439295 at epoch 76\n",
      "Error: 0.012516968063908073 at epoch 77\n",
      "Error: 0.012486021052834875 at epoch 78\n",
      "Error: 0.01245543438513409 at epoch 79\n",
      "Error: 0.012425181615437058 at epoch 80\n",
      "Error: 0.012395235882394634 at epoch 81\n",
      "Error: 0.01236557017339214 at epoch 82\n",
      "Error: 0.012336157554372649 at epoch 83\n",
      "Error: 0.012306971366761867 at epoch 84\n",
      "Error: 0.012277985394297898 at epoch 85\n",
      "Error: 0.012249174002986722 at epoch 86\n",
      "Error: 0.01222051225752285 at epoch 87\n",
      "Error: 0.012191976017415562 at epoch 88\n",
      "Error: 0.012163542015816013 at epoch 89\n",
      "Error: 0.012135187923705524 at epoch 90\n",
      "Error: 0.012106892401726506 at epoch 91\n",
      "Error: 0.01207863514154733 at epoch 92\n",
      "Error: 0.012050396898277943 at epoch 93\n",
      "Error: 0.012022159515108398 at epoch 94\n",
      "Error: 0.01199390594104216 at epoch 95\n",
      "Error: 0.011965620242342911 at epoch 96\n",
      "Error: 0.011937287608114359 at epoch 97\n",
      "Error: 0.011908894350282308 at epoch 98\n",
      "Error: 0.011880427898149712 at epoch 99\n",
      "Error: 0.011851876787640258 at epoch 100\n",
      "Training complete!\n",
      "=====\n",
      "\n",
      "Précision sur l'ensemble de test: 96.67%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "# Votre code MLP ici\n",
    "# (coller votre classe MLP ici)\n",
    "\n",
    "# Étape 1 : Charger et préparer les données\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Charger les caractéristiques (features) et les étiquettes (labels)\n",
    "X = iris.data  # Caractéristiques\n",
    "y = iris.target.reshape(-1, 1)  # Labels\n",
    "\n",
    "# Normaliser les caractéristiques (X) entre 0 et 1\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Appliquer one-hot encoding aux labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Diviser le jeu de données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2)\n",
    "\n",
    "# Étape 2 : Créer et entraîner le MLP\n",
    "mlp = MLP(num_inputs=4, hidden_layers=[5], num_outputs=3)  # 4 entrées, 1 couche cachée avec 5 neurones, 3 sorties\n",
    "mlp.train(X_train, y_train, epochs=100, learning_rate=0.1)\n",
    "\n",
    "# Étape 3 : Tester le MLP sur les données de test\n",
    "correct = 0\n",
    "for i, input in enumerate(X_test):\n",
    "    # Faire une prédiction\n",
    "    output = mlp.forward_propagate(input)\n",
    "    \n",
    "    # Trouver l'index de la classe prédite (avec la plus grande probabilité)\n",
    "    predicted_class = np.argmax(output)\n",
    "    \n",
    "    # Trouver l'index de la vraie classe\n",
    "    true_class = np.argmax(y_test[i])\n",
    "    \n",
    "    if predicted_class == true_class:\n",
    "        correct += 1\n",
    "\n",
    "# Afficher les résultats\n",
    "print()\n",
    "print(\"Précision sur l'ensemble de test: {:.2f}%\".format((correct / len(X_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38888889 1.         0.08474576 0.125     ] [1. 0. 0.]\n",
      "Class prédite 0  vrai classe 0\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "nb = randint(1,29)\n",
    "print(X_test[nb], y_test[nb])\n",
    "output = mlp.forward_propagate(X_test[nb])\n",
    "predicted_class = np.argmax(output)\n",
    "true_class = np.argmax(y_test[nb])\n",
    "print(\"Class prédite\", predicted_class, \" vrai classe\", true_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
